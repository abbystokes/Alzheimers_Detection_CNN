{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNeurodCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (adaptive_pool): AdaptiveAvgPool2d(output_size=(44, 44))\n",
      "  (fc1): Linear(in_features=123904, out_features=500, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=500, out_features=3, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 178, 178]             320\n",
      "         MaxPool2d-2           [-1, 32, 89, 89]               0\n",
      "            Conv2d-3           [-1, 32, 87, 87]           9,248\n",
      "            Conv2d-4           [-1, 64, 85, 85]          18,496\n",
      " AdaptiveAvgPool2d-5           [-1, 64, 44, 44]               0\n",
      "            Linear-6                  [-1, 500]      61,952,500\n",
      "           Dropout-7                  [-1, 500]               0\n",
      "            Linear-8                    [-1, 3]           1,503\n",
      "================================================================\n",
      "Total params: 61,982,067\n",
      "Trainable params: 61,982,067\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.12\n",
      "Forward/backward pass size (MB): 16.00\n",
      "Params size (MB): 236.44\n",
      "Estimated Total Size (MB): 252.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from src.data.dataset import load_dataset\n",
    "from src.models.models import SNeurodCNN\n",
    "\n",
    "class SNeurodCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SNeurodCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((44, 44))\n",
    "        self.fc1 = nn.Linear(64 * 44 * 44, 500)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(500, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.adaptive_pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 44 * 44) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model and print its structure\n",
    "model = SNeurodCNN()\n",
    "print(model)\n",
    "summary(model, input_size=(1, 180, 180))\n",
    "PATH = './models/sneurod_cnn.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tried using Aahan's Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.data.dataset import load_dataset\n",
    "from src.models.train import train\n",
    "from src.models.test import test\n",
    "\n",
    "classes = ('CN', 'AD', 'MCI')\n",
    "\n",
    "id2label = {i: classes[i] for i in range(len(classes))}\n",
    "label2id = {classes[i]: i for i in range(len(classes))}\n",
    "\n",
    "trainset, testset, valset = load_dataset(label2id=label2id)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 3.846\n",
      "[1,    20] loss: 1.804\n",
      "[2,    10] loss: 1.248\n",
      "[2,    20] loss: 1.089\n",
      "[3,    10] loss: 1.031\n",
      "[3,    20] loss: 1.020\n",
      "[4,    10] loss: 0.978\n",
      "[4,    20] loss: 0.971\n",
      "[5,    10] loss: 0.907\n",
      "[5,    20] loss: 0.854\n",
      "[6,    10] loss: 0.763\n",
      "[6,    20] loss: 0.737\n",
      "[7,    10] loss: 0.662\n",
      "[7,    20] loss: 0.774\n",
      "[8,    10] loss: 0.542\n",
      "[8,    20] loss: 0.508\n",
      "[9,    10] loss: 0.424\n",
      "[9,    20] loss: 0.455\n",
      "[10,    10] loss: 0.352\n",
      "[10,    20] loss: 0.365\n",
      "[11,    10] loss: 0.259\n",
      "[11,    20] loss: 0.274\n",
      "[12,    10] loss: 0.204\n",
      "[12,    20] loss: 0.240\n",
      "[13,    10] loss: 0.184\n",
      "[13,    20] loss: 0.160\n",
      "[14,    10] loss: 0.130\n",
      "[14,    20] loss: 0.131\n",
      "[15,    10] loss: 0.095\n",
      "[15,    20] loss: 0.109\n",
      "[16,    10] loss: 0.064\n",
      "[16,    20] loss: 0.057\n",
      "[17,    10] loss: 0.058\n",
      "[17,    20] loss: 0.052\n",
      "[18,    10] loss: 0.069\n",
      "[18,    20] loss: 0.063\n",
      "[19,    10] loss: 0.067\n",
      "[19,    20] loss: 0.071\n",
      "[20,    10] loss: 0.060\n",
      "[20,    20] loss: 0.066\n",
      "[21,    10] loss: 0.057\n",
      "[21,    20] loss: 0.045\n",
      "[22,    10] loss: 0.051\n",
      "[22,    20] loss: 0.031\n",
      "[23,    10] loss: 0.034\n",
      "[23,    20] loss: 0.028\n",
      "[24,    10] loss: 0.042\n",
      "[24,    20] loss: 0.036\n",
      "[25,    10] loss: 0.029\n",
      "[25,    20] loss: 0.042\n",
      "[26,    10] loss: 0.029\n",
      "[26,    20] loss: 0.020\n",
      "[27,    10] loss: 0.017\n",
      "[27,    20] loss: 0.019\n",
      "[28,    10] loss: 0.033\n",
      "[28,    20] loss: 0.028\n",
      "[29,    10] loss: 0.033\n",
      "[29,    20] loss: 0.031\n",
      "[30,    10] loss: 0.017\n",
      "[30,    20] loss: 0.015\n",
      "[31,    10] loss: 0.013\n",
      "[31,    20] loss: 0.008\n",
      "[32,    10] loss: 0.007\n",
      "[32,    20] loss: 0.007\n",
      "[33,    10] loss: 0.007\n",
      "[33,    20] loss: 0.011\n",
      "[34,    10] loss: 0.008\n",
      "[34,    20] loss: 0.007\n",
      "[35,    10] loss: 0.019\n",
      "[35,    20] loss: 0.011\n",
      "[36,    10] loss: 0.009\n",
      "[36,    20] loss: 0.010\n",
      "[37,    10] loss: 0.013\n",
      "[37,    20] loss: 0.014\n",
      "[38,    10] loss: 0.011\n",
      "[38,    20] loss: 0.010\n",
      "[39,    10] loss: 0.008\n",
      "[39,    20] loss: 0.006\n",
      "[40,    10] loss: 0.005\n",
      "[40,    20] loss: 0.006\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Module.load_state_dict of SNeurodCNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (adaptive_pool): AdaptiveAvgPool2d(output_size=(44, 44))\n",
       "  (fc1): Linear(in_features=123904, out_features=500, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=500, out_features=3, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from src.data.dataset import load_dataset\n",
    "from src.models.models import SNeurodCNN\n",
    "\n",
    "net = SNeurodCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "max_epochs = 40\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs.float())\n",
    "            loss = criterion(outputs, torch.Tensor(labels))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 9:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}')\n",
    "                if running_loss < 0.01:\n",
    "                     break\n",
    "                running_loss = 0.0\n",
    "\n",
    "       # if epoch % 10 == 9:\n",
    "       #     # Saving the final epoch of the model\n",
    "       #     PATH = f'./models/sneurod_cnn_{epoch+1}.pth'\n",
    "       #     torch.save(net.state_dict(), PATH)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "model.load_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88        37\n",
      "           1       0.94      0.81      0.87        37\n",
      "           2       0.86      0.91      0.89        34\n",
      "\n",
      "    accuracy                           0.88       108\n",
      "   macro avg       0.88      0.88      0.88       108\n",
      "weighted avg       0.88      0.88      0.88       108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.models.models import SNeurodCNN\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def test(testloader):\n",
    "    #PATH = './models/sneurod_cnn_10.pth'\n",
    "    #net = SNeurodCNN()\n",
    "    #net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images.float())\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            y_true.extend(labels)\n",
    "            y_pred.extend(predicted)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # print(f'Accuracy of the network on the 1000 test images: {100 * correct // total} %')\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "test(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.87       321\n",
      "           1       0.90      0.82      0.86       254\n",
      "           2       0.84      0.90      0.87       280\n",
      "\n",
      "    accuracy                           0.87       855\n",
      "   macro avg       0.87      0.87      0.87       855\n",
      "weighted avg       0.87      0.87      0.87       855\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
